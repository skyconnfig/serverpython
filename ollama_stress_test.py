import ollama
import threading
import time
import json
import csv
import matplotlib.pyplot as plt
import pandas as pd
import psutil
import pynvml
from datetime import datetime
plt.rcParams['font.sans-serif'] = ['SimHei']  # æˆ– ['Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå· '-' æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜

# æµ‹è¯•é…ç½®
MODEL_NAME = "deepseek-r1:32b"  # è¦æµ‹è¯•çš„æ¨¡å‹
TEST_DURATION = 600                # æµ‹è¯•æ—¶é•¿(ç§’)
CONCURRENT_THREADS = 8             # å¹¶å‘çº¿ç¨‹æ•°
PROMPT_FILE = "D:\\hework\\debug\\prompts.txt"        # æç¤ºè¯æ–‡ä»¶
OUTPUT_PREFIX = f"ollama_stress_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
TEMPERATURE = 0.7
MAX_TOKENS = 512

# å…¨å±€ç»Ÿè®¡
stats = {
    "start_time": time.time(),
    "total_requests": 0,
    "successful_requests": 0,
    "failed_requests": 0,
    "total_tokens": 0,
    "total_latency": 0.0,
    "request_history": [],
    "system_stats": []
}

# åˆå§‹åŒ–GPUç›‘æ§
pynvml.nvmlInit()
gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)

def get_system_stats():
    """è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
    # CPUä½¿ç”¨ç‡
    cpu_percent = psutil.cpu_percent()
    
    # å†…å­˜ä½¿ç”¨
    mem = psutil.virtual_memory()
    
    # GPUä½¿ç”¨
    gpu_util = pynvml.nvmlDeviceGetUtilizationRates(gpu_handle).gpu
    gpu_mem_info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)
    gpu_mem_percent = (gpu_mem_info.used / gpu_mem_info.total) * 100
    gpu_temp = pynvml.nvmlDeviceGetTemperature(gpu_handle, pynvml.NVML_TEMPERATURE_GPU)
    
    return {
        "timestamp": datetime.now().isoformat(),
        "cpu_percent": cpu_percent,
        "mem_percent": mem.percent,
        "mem_used_gb": mem.used / (1024 ** 3),
        "gpu_util": gpu_util,
        "gpu_mem_percent": gpu_mem_percent,
        "gpu_temp": gpu_temp
    }

def load_prompts():
    """åŠ è½½æç¤ºè¯æ–‡ä»¶"""
    try:
        with open(PROMPT_FILE, "r", encoding="utf-8") as f:
            return [line.strip() for line in f.readlines() if line.strip()]
    except FileNotFoundError:
        print(f"æç¤ºè¯æ–‡ä»¶ {PROMPT_FILE} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤æç¤º")
        return [
            "è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†",
            "ç”¨Pythonå®ç°å¿«é€Ÿæ’åºç®—æ³•",
            "å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½æœªæ¥å‘å±•çš„çŸ­æ–‡",
            "å¦‚ä½•ä¼˜åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦?",
            "æ¯”è¾ƒRESTful APIå’ŒGraphQLçš„ä¼˜ç¼ºç‚¹"
        ]

def worker(worker_id, prompts):
    """å‹åŠ›æµ‹è¯•å·¥ä½œçº¿ç¨‹"""
    prompt_index = 0
    while time.time() - stats["start_time"] < TEST_DURATION:
        prompt = prompts[prompt_index % len(prompts)]
        prompt_index += 1
        
        try:
            start_time = time.time()
            
            # å‘é€æ¨¡å‹è¯·æ±‚
            response = ollama.generate(
                model=MODEL_NAME,
                prompt=prompt,
                options={
                    "temperature": TEMPERATURE,
                    "num_predict": MAX_TOKENS
                }
            )
            
            latency = time.time() - start_time
            tokens_generated = len(response["response"].split())
            
            # æ›´æ–°ç»Ÿè®¡
            with threading.Lock():
                stats["total_requests"] += 1
                stats["successful_requests"] += 1
                stats["total_tokens"] += tokens_generated
                stats["total_latency"] += latency
                
                stats["request_history"].append({
                    "timestamp": datetime.now().isoformat(),
                    "worker_id": worker_id,
                    "prompt": prompt[:50] + "..." if len(prompt) > 50 else prompt,
                    "latency": latency,
                    "tokens": tokens_generated,
                    "success": True
                })
                
        except Exception as e:
            with threading.Lock():
                stats["total_requests"] += 1
                stats["failed_requests"] += 1
                stats["request_history"].append({
                    "timestamp": datetime.now().isoformat(),
                    "worker_id": worker_id,
                    "prompt": prompt[:50] + "..." if len(prompt) > 50 else prompt,
                    "error": str(e),
                    "success": False
                })

def monitor():
    """ç³»ç»Ÿç›‘æ§çº¿ç¨‹"""
    while time.time() - stats["start_time"] < TEST_DURATION:
        time.sleep(5)
        with threading.Lock():
            stats["system_stats"].append(get_system_stats())

def reporter():
    """å®æ—¶æŠ¥å‘Šçº¿ç¨‹"""
    start_time = stats["start_time"]
    last_tokens = 0
    last_requests = 0
    
    while time.time() - start_time < TEST_DURATION:
        time.sleep(10)
        elapsed = time.time() - start_time
        minutes = int(elapsed // 60)
        seconds = int(elapsed % 60)
        
        with threading.Lock():
            current_tokens = stats["total_tokens"]
            current_requests = stats["successful_requests"]
            
            # è®¡ç®—å½“å‰åŒºé—´TPSå’ŒRPS
            interval_tokens = current_tokens - last_tokens
            interval_requests = current_requests - last_requests
            tps = interval_tokens / 10
            rps = interval_requests / 10
            
            last_tokens = current_tokens
            last_requests = current_requests
            
            # è·å–æœ€æ–°çš„ç³»ç»ŸçŠ¶æ€
            if stats["system_stats"]:
                sys_stat = stats["system_stats"][-1]
                gpu_temp = sys_stat["gpu_temp"]
                gpu_util = sys_stat["gpu_util"]
                gpu_mem = sys_stat["gpu_mem_percent"]
            else:
                gpu_temp = gpu_util = gpu_mem = 0
            
            print(
                f"[{minutes:02d}:{seconds:02d}] "
                f"Req: {stats['successful_requests']}/{stats['total_requests']} "
                f"(å¤±è´¥: {stats['failed_requests']}) | "
                f"TPS: {tps:.1f} | "
                f"RPS: {rps:.1f} | "
                f"å»¶è¿Ÿ: { 0 if stats['successful_requests'] == 0 else (stats['total_latency']/stats['successful_requests'])*1000:.1f}ms |"
                f"GPU: {gpu_util}%/{gpu_temp}Â°C | "
                f"æ˜¾å­˜: {gpu_mem:.1f}%"
            )

def save_results():
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    # ä¿å­˜è¯·æ±‚å†å²
    with open(f"{OUTPUT_PREFIX}_requests.csv", "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["timestamp", "worker_id", "prompt", "latency", "tokens", "success", "error"])
        writer.writeheader()
        writer.writerows(stats["request_history"])
    
    # ä¿å­˜ç³»ç»ŸçŠ¶æ€
    with open(f"{OUTPUT_PREFIX}_system.csv", "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=stats["system_stats"][0].keys())
        writer.writeheader()
        writer.writerows(stats["system_stats"])
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    total_time = TEST_DURATION
    avg_tps = stats["total_tokens"] / total_time
    avg_rps = stats["successful_requests"] / total_time
    avg_latency = (stats["total_latency"] / stats["successful_requests"]) * 1000 if stats["successful_requests"] > 0 else 0
    success_rate = (stats["successful_requests"] / stats["total_requests"]) * 100 if stats["total_requests"] > 0 else 0
    
    report = {
        "model": MODEL_NAME,
        "test_duration": TEST_DURATION,
        "concurrent_threads": CONCURRENT_THREADS,
        "start_time": datetime.fromtimestamp(stats["start_time"]).isoformat(),
        "end_time": datetime.now().isoformat(),
        "total_requests": stats["total_requests"],
        "successful_requests": stats["successful_requests"],
        "failed_requests": stats["failed_requests"],
        "success_rate": f"{success_rate:.2f}%",
        "total_tokens": stats["total_tokens"],
        "avg_tps": f"{avg_tps:.2f}",
        "avg_rps": f"{avg_rps:.2f}",
        "avg_latency_ms": f"{avg_latency:.1f}",
        "max_gpu_temp": max(s["gpu_temp"] for s in stats["system_stats"]),
        "max_gpu_util": max(s["gpu_util"] for s in stats["system_stats"]),
        "max_gpu_mem": max(s["gpu_mem_percent"] for s in stats["system_stats"])
    }
    
    with open(f"{OUTPUT_PREFIX}_report.json", "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    return report

def generate_visual_report(report):
    """ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š"""
    # è¯»å–æ•°æ®
    requests_df = pd.read_csv(f"{OUTPUT_PREFIX}_requests.csv")
    system_df = pd.read_csv(f"{OUTPUT_PREFIX}_system.csv")
    
    # è½¬æ¢æ—¶é—´æˆ³
    requests_df["timestamp"] = pd.to_datetime(requests_df["timestamp"])
    system_df["timestamp"] = pd.to_datetime(system_df["timestamp"])
    
    # è®¾ç½®æ—¶é—´ç´¢å¼•
    requests_df.set_index("timestamp", inplace=True)
    system_df.set_index("timestamp", inplace=True)
    
    # åˆ›å»ºå›¾è¡¨
    plt.figure(figsize=(15, 12))
    
    # TPS å›¾è¡¨
    plt.subplot(3, 1, 1)
    requests_df["tokens"].resample("10S").sum().plot(label="Tokens/10s")
    plt.ylabel("Tokens")
    plt.title(f"Ollama å‹åŠ›æµ‹è¯•æŠ¥å‘Š - {MODEL_NAME}\nå¹³å‡TPS: {report['avg_tps']} | å¹³å‡å»¶è¿Ÿ: {report['avg_latency_ms']}ms")
    plt.legend()
    plt.grid(True)
    
    # è¯·æ±‚æˆåŠŸç‡å›¾è¡¨
    plt.subplot(3, 1, 2)
    requests_df["success"].resample("10S").mean().plot(label="æˆåŠŸç‡", color="green")
    plt.axhline(y=0.95, color="r", linestyle="--", label="95%è­¦æˆ’çº¿")
    plt.ylabel("æˆåŠŸç‡")
    plt.ylim(0.7, 1.05)
    plt.legend()
    plt.grid(True)
    
    # ç³»ç»Ÿèµ„æºå›¾è¡¨
    plt.subplot(3, 1, 3)
    system_df["gpu_util"].plot(label="GPUåˆ©ç”¨ç‡", color="blue")
    system_df["gpu_temp"].plot(label="GPUæ¸©åº¦", secondary_y=True, color="red")
    plt.axhline(y=85, color="r", linestyle="--", label="æ¸©åº¦è­¦æˆ’çº¿")
    plt.ylabel("åˆ©ç”¨ç‡ (%)")
    plt.xlabel("æ—¶é—´")
    plt.legend(loc="upper left")
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_PREFIX}_report.png", dpi=150)

def main():
    # åŠ è½½æç¤ºè¯
    prompts = load_prompts()
    print(f"âœ… å·²åŠ è½½ {len(prompts)} æ¡æç¤ºè¯")
    
    # å¯åŠ¨å·¥ä½œçº¿ç¨‹
    threads = []
    for i in range(CONCURRENT_THREADS):
        t = threading.Thread(target=worker, args=(i, prompts))
        t.start()
        threads.append(t)
    
    # å¯åŠ¨ç›‘æ§å’ŒæŠ¥å‘Šçº¿ç¨‹
    threading.Thread(target=monitor, daemon=True).start()
    threading.Thread(target=reporter, daemon=True).start()
    
    # æ˜¾ç¤ºæµ‹è¯•ä¿¡æ¯
    print(f"ğŸš€ å¼€å§‹å‹åŠ›æµ‹è¯•: {CONCURRENT_THREADS}çº¿ç¨‹, æŒç»­{TEST_DURATION//60}åˆ†é’Ÿ")
    print(f"æ¨¡å‹: {MODEL_NAME}")
    print("-" * 80)
    
    # ç­‰å¾…æµ‹è¯•ç»“æŸ
    time.sleep(TEST_DURATION)
    
    # ä¿å­˜ç»“æœ
    print("\nğŸ“Š æµ‹è¯•å®Œæˆï¼Œä¿å­˜ç»“æœ...")
    report = save_results()
    
    # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
    generate_visual_report(report)
    print(f"ğŸ“ˆ æŠ¥å‘Šå·²ç”Ÿæˆ: {OUTPUT_PREFIX}_report.png")
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 80)
    print(f"æ¨¡å‹: {report['model']}")
    print(f"æŒç»­æ—¶é—´: {report['test_duration']}ç§’")
    print(f"å¹¶å‘æ•°: {report['concurrent_threads']}")
    print(f"æ€»è¯·æ±‚æ•°: {report['total_requests']} (æˆåŠŸ: {report['successful_requests']}, å¤±è´¥: {report['failed_requests']})")
    print(f"æˆåŠŸç‡: {report['success_rate']}")
    print(f"æ€»Tokenæ•°: {report['total_tokens']}")
    print(f"å¹³å‡TPS: {report['avg_tps']}")
    print(f"å¹³å‡RPS: {report['avg_rps']}")
    print(f"å¹³å‡å»¶è¿Ÿ: {report['avg_latency_ms']}ms")
    print(f"æœ€å¤§GPUæ¸©åº¦: {report['max_gpu_temp']}Â°C")
    print(f"æœ€å¤§GPUåˆ©ç”¨ç‡: {report['max_gpu_util']}%")
    print(f"æœ€å¤§æ˜¾å­˜ä½¿ç”¨ç‡: {report['max_gpu_mem']}%")
    print("=" * 80)

if __name__ == "__main__":
    main()